{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete BombCell + UnitMatch Pipeline Demo\n",
    "\n",
    "This notebook demonstrates the complete pipeline for neural spike analysis using BombCell and UnitMatch:\n",
    "\n",
    "## 🎯 **What This Demo Does**\n",
    "\n",
    "### Part 1: BombCell Quality Control\n",
    "- Analyzes spike sorting data from Kilosort\n",
    "- Extracts comprehensive quality metrics for each unit\n",
    "- Classifies units as \"GOOD\", \"MUA\", \"NOISE\", or \"NON-SOMA\"  \n",
    "- **Creates waveform data specifically formatted for UnitMatch**\n",
    "\n",
    "### Part 2: UnitMatch Cross-Session Tracking\n",
    "- Uses BombCell outputs to track neurons across recording sessions\n",
    "- Applies machine learning to match units based on waveform features\n",
    "- Provides interactive tools for manual curation\n",
    "- Generates probability matrices showing match confidence\n",
    "\n",
    "## 🔗 **Key Integration Points**\n",
    "- BombCell creates `RawWaveforms/` folders that UnitMatch requires\n",
    "- Special parameters optimize BombCell for UnitMatch compatibility\n",
    "- Quality classifications help filter units for reliable matching\n",
    "- Both tools work seamlessly with standard Kilosort outputs\n",
    "\n",
    "## 📋 **What You Need**\n",
    "- Kilosort output directories from 2+ recording sessions if you want to track across days or for 1 recording session if you want to merge units intra-session\n",
    "- Access to raw `.bin` files (for waveform extraction)\n",
    "- Meta files (`.meta` for SpikeGLX or `.oebin` for Open Ephys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ BombCell and UnitMatch imported successfully\n",
      "🚀 Ready to analyze neural data!\n"
     ]
    }
   ],
   "source": [
    "# 📦 Import Required Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import BombCell for quality control\n",
    "import bombcell as bc\n",
    "\n",
    "# Import UnitMatch for cross-session tracking (optional - only if running Part 2)\n",
    "try:\n",
    "    import UnitMatchPy.bayes_functions as bf\n",
    "    import UnitMatchPy.utils as util\n",
    "    import UnitMatchPy.overlord as ov\n",
    "    import UnitMatchPy.save_utils as su\n",
    "    import UnitMatchPy.GUI as um_gui\n",
    "    import UnitMatchPy.assign_unique_id as aid\n",
    "    import UnitMatchPy.default_params as default_params\n",
    "    UNITMATCH_AVAILABLE = True\n",
    "    print(\"✅ BombCell and UnitMatch imported successfully\")\n",
    "except ImportError as e:\n",
    "    UNITMATCH_AVAILABLE = False\n",
    "    print(\"✅ BombCell imported successfully\")\n",
    "    print(\"⚠️  UnitMatch not available - please install: pip install UnitMatchPy\")\n",
    "    print(f\"    Error: {e}\")\n",
    "\n",
    "print(\"🚀 Ready to analyze neural data!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📁 Part 1: Configure Data Paths for BombCell\n",
    "\n",
    "**Edit these paths to point to your actual data:**\n",
    "\n",
    "For this demo, we'll analyze data from multiple sessions to demonstrate the complete pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 BombCell + UnitMatch Pipeline Demo\n",
      "📊 Dataset: calca_302 cross-session tracking\n",
      "🔬 Sessions to analyze: 2\n",
      "   Session 1: calca_302_2023-04-19\n",
      "      📁 KS: kilosort4\n",
      "   Session 2: calca_302_2023-04-20\n",
      "      📁 KS: kilosort4\n",
      "\n",
      "🔧 Kilosort version: 4\n",
      "🎯 This demo will run BombCell first, then use outputs for UnitMatch tracking\n"
     ]
    }
   ],
   "source": [
    "# 📁 Configure Data Paths - Using Processing Playground Dataset\n",
    "\n",
    "# These are the exact paths from the processing_playground for testing\n",
    "# calca_302 sessions from 2023-04-19 and 2023-04-20\n",
    "\n",
    "session_configs = [\n",
    "    {\n",
    "        'name': 'calca_302_2023-04-19',\n",
    "        'ks_dir': r'/home/jf5479/cup/Chris/data/cta_backwards/calca_302/2023-04-19/cz_npxl_g0/cz_npxl_g0_imec0/kilosort4',\n",
    "        'raw_file': r'/home/jf5479/cup/Chris/data/cta_backwards/calca_302/2023-04-19/cz_npxl_g0/cz_npxl_g0_imec0/cz_npxl_g0_t0.imec0.ap.bin',\n",
    "        'meta_file': r'/home/jf5479/cup/Chris/data/cta_backwards/calca_302/2023-04-19/cz_npxl_g0/cz_npxl_g0_imec0/cz_npxl_g0_t0.imec0.ap.meta'\n",
    "    },\n",
    "    {\n",
    "        'name': 'calca_302_2023-04-20', \n",
    "        'ks_dir': r'/home/jf5479/cup/Chris/data/cta_backwards/calca_302/2023-04-20/cz_npxl_g0/cz_npxl_g0_imec0/kilosort4',\n",
    "        'raw_file': r'/home/jf5479/cup/Chris/data/cta_backwards/calca_302/2023-04-20/cz_npxl_g0/cz_npxl_g0_imec0/cz_npxl_g0_t0.imec0.ap.bin',\n",
    "        'meta_file': r'/home/jf5479/cup/Chris/data/cta_backwards/calca_302/2023-04-20/cz_npxl_g0/cz_npxl_g0_imec0/cz_npxl_g0_t0.imec0.ap.meta'\n",
    "    }\n",
    "]\n",
    "\n",
    "kilosort_version = 4\n",
    "\n",
    "print(\"🎯 BombCell + UnitMatch Pipeline Demo\")\n",
    "print(\"📊 Dataset: calca_302 cross-session tracking\")\n",
    "print(f\"🔬 Sessions to analyze: {len(session_configs)}\")\n",
    "for i, config in enumerate(session_configs):\n",
    "    print(f\"   Session {i+1}: {config['name']}\")\n",
    "    print(f\"      📁 KS: {Path(config['ks_dir']).name}\")\n",
    "    \n",
    "print(f\"\\n🔧 Kilosort version: {kilosort_version}\")\n",
    "print(\"🎯 This demo will run BombCell first, then use outputs for UnitMatch tracking\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧪 Part 1: Run BombCell Quality Control\n",
    "\n",
    "**What BombCell does:**\n",
    "- Analyzes each unit's waveform properties, firing patterns, and spatial characteristics\n",
    "- Calculates 15+ quality metrics (amplitude, drift, contamination, etc.)\n",
    "- Classifies units into categories based on quality thresholds\n",
    "- **Special for UnitMatch**: Extracts 1000 raw spikes per unit (vs. standard 100)\n",
    "\n",
    "**Key UnitMatch optimizations:**\n",
    "- `detrendWaveform=False`: Preserves raw waveform shape for matching\n",
    "- `nRawSpikesToExtract=1000`: More spikes = better cross-validation\n",
    "- `saveMultipleRaw=True`: Saves separate waveform sets for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Running BombCell analysis on all sessions...\n",
      "\n",
      "============================================================\n",
      "SESSION 1/2\n",
      "============================================================\n",
      "🔬 Analyzing session: calca_302_2023-04-19\n",
      "   📁 Kilosort directory: /home/jf5479/cup/Chris/data/cta_backwards/calca_302/2023-04-19/cz_npxl_g0/cz_npxl_g0_imec0/kilosort4\n",
      "   🗂️  Raw file: cz_npxl_g0_t0.imec0.ap.bin\n",
      "   📄 Meta file: cz_npxl_g0_t0.imec0.ap.meta\n",
      "   💾 Results will be saved to: /home/jf5479/cup/Chris/data/cta_backwards/calca_302/2023-04-19/cz_npxl_g0/cz_npxl_g0_imec0/kilosort4/bombcell_testing_jf\n",
      "   🚀 Running BombCell analysis...\n",
      "Using raw data cz_npxl_g0_t0.imec0.ap.bin.\n",
      "🚀 Running BombCell with UnitMatch parameters...\n",
      "   - Extracting 1000 raw spikes per unit\n",
      "   - Saving multiple raw waveforms: True\n",
      "   - Detrending waveforms: True\n",
      "🚀 Starting BombCell quality metrics pipeline...\n",
      "📁 Processing data from: /home/jf5479/cup/Chris/data/cta_backwards/calca_302/2023-04-19/cz_npxl_g0/cz_npxl_g0_imec0/kilosort4\n",
      "Results will be saved to: /home/jf5479/cup/Chris/data/cta_backwards/calca_302/2023-04-19/cz_npxl_g0/cz_npxl_g0_imec0/kilosort4/bombcell_testing_jf\n",
      "\n",
      "Loading ephys data...\n",
      "Loaded ephys data: 657 units, 12,878,168 spikes\n",
      "\n",
      "🔍 Extracting raw waveforms...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e917a5dbf59c44e281b40de317854f03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    }
   ],
   "source": [
    "def run_bombcell_session(session_config):\n",
    "    \"\"\"Run BombCell on a single session with UnitMatch parameters\"\"\"\n",
    "    \n",
    "    name = session_config['name']\n",
    "    ks_dir = session_config['ks_dir']\n",
    "    raw_file = session_config.get('raw_file')\n",
    "    meta_file = session_config.get('meta_file')\n",
    "    \n",
    "    # Create save path in the kilosort directory\n",
    "    save_path = Path(ks_dir) / \"bombcell_testing_jf\"\n",
    "    \n",
    "    print(f\"🔬 Analyzing session: {name}\")\n",
    "    print(f\"   📁 Kilosort directory: {ks_dir}\")\n",
    "    print(f\"   🗂️  Raw file: {Path(raw_file).name if raw_file else 'Not specified'}\")\n",
    "    print(f\"   📄 Meta file: {Path(meta_file).name if meta_file else 'Not specified'}\")\n",
    "    print(f\"   💾 Results will be saved to: {save_path}\")\n",
    "    \n",
    "    # Check if BombCell has already been run\n",
    "    existing_results = save_path / \"cluster_bc_unitType.tsv\"\n",
    "    if existing_results.exists():\n",
    "        print(f\"   ✅ Found existing BombCell results - loading from disk\")\n",
    "        \n",
    "        # Load existing results\n",
    "        try:\n",
    "            param, quality_metrics, unit_type_string = bc.load_bc_results(str(save_path))\n",
    "            unit_type = np.array([1 if ut == 'GOOD' else 0 for ut in unit_type_string])\n",
    "            \n",
    "            print(f\"   📊 Loaded results - Total units: {len(unit_type)}\")\n",
    "            print(f\"   🟢 Good units: {np.sum(unit_type_string == 'GOOD')}\")\n",
    "            print(f\"   🟡 MUA units: {np.sum(unit_type_string == 'MUA')}\")\n",
    "            print(f\"   🔴 Noise units: {np.sum(unit_type_string == 'NOISE')}\")\n",
    "            print(f\"   ⚫ Non-somatic: {np.sum(unit_type_string == 'NON-SOMA')}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ⚠️  Error loading existing results: {e}\")\n",
    "            print(f\"   🔄 Will re-run BombCell analysis...\")\n",
    "            existing_results = None\n",
    "    else:\n",
    "        existing_results = None\n",
    "    \n",
    "    # Run BombCell if no existing results or loading failed\n",
    "    if not existing_results or not existing_results.exists():\n",
    "        print(\"   🚀 Running BombCell analysis...\")\n",
    "        quality_metrics, param, unit_type, unit_type_string = bc.run_bombcell_unit_match(\n",
    "            ks_dir=ks_dir,\n",
    "            save_path=str(save_path),\n",
    "            raw_file=raw_file,\n",
    "            meta_file=meta_file,\n",
    "            kilosort_version=kilosort_version\n",
    "        )\n",
    "        \n",
    "        print(f\"   ✅ Analysis complete!\")\n",
    "        print(f\"   📊 Total units: {len(unit_type)}\")\n",
    "        print(f\"   🟢 Good units: {np.sum(unit_type_string == 'GOOD')}\")\n",
    "        print(f\"   🟡 MUA units: {np.sum(unit_type_string == 'MUA')}\")\n",
    "        print(f\"   🔴 Noise units: {np.sum(unit_type_string == 'NOISE')}\")\n",
    "        print(f\"   ⚫ Non-somatic: {np.sum(unit_type_string == 'NON-SOMA')}\")\n",
    "    \n",
    "    # Check UnitMatch waveforms\n",
    "    raw_waveforms_dir = save_path / \"RawWaveforms\"\n",
    "    if raw_waveforms_dir.exists():\n",
    "        waveform_files = list(raw_waveforms_dir.glob(\"Unit*_RawSpikes.npy\"))\n",
    "        print(f\"   🎯 UnitMatch waveforms: {len(waveform_files)} files saved\")\n",
    "    else:\n",
    "        print(f\"   ⚠️  No UnitMatch waveforms - check raw file access\")\n",
    "    \n",
    "    return {\n",
    "        'name': name,\n",
    "        'ks_dir': ks_dir,\n",
    "        'save_path': str(save_path),\n",
    "        'quality_metrics': quality_metrics,\n",
    "        'param': param,\n",
    "        'unit_type': unit_type,\n",
    "        'unit_type_string': unit_type_string\n",
    "    }\n",
    "\n",
    "# Run BombCell on all sessions\n",
    "print(\"🚀 Running BombCell analysis on all sessions...\")\n",
    "session_results = []\n",
    "\n",
    "for i, session_config in enumerate(session_configs):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"SESSION {i+1}/{len(session_configs)}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    result = run_bombcell_session(session_config)\n",
    "    session_results.append(result)\n",
    "\n",
    "print(f\"\\n🎉 BombCell analysis complete for {len(session_results)} sessions!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📊 Part 1 Results: BombCell Quality Metrics Summary\n",
    "\n",
    "Let's examine the quality control results and understand what BombCell found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📈 Analyze BombCell Results\n",
    "for i, result in enumerate(session_results):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"📊 SESSION {i+1}: {Path(result['ks_dir']).name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Convert to DataFrame for easier analysis\n",
    "    df = pd.DataFrame(result['quality_metrics'])\n",
    "    df['unit_type'] = result['unit_type_string']\n",
    "    df['cluster_id'] = df['phy_clusterID'].astype(int)\n",
    "    \n",
    "    # Unit type summary\n",
    "    print(\"\\n🏷️  Unit Classifications:\")\n",
    "    type_counts = df['unit_type'].value_counts()\n",
    "    for unit_type, count in type_counts.items():\n",
    "        percentage = (count / len(df)) * 100\n",
    "        print(f\"   {unit_type}: {count} units ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Key quality metrics summary\n",
    "    print(f\"\\n🔍 Quality Metrics Summary:\")\n",
    "    good_units = df[df['unit_type'] == 'GOOD']\n",
    "    if len(good_units) > 0:\n",
    "        print(f\"   📶 Amplitude (good units): {good_units['amplitude'].mean():.1f} ± {good_units['amplitude'].std():.1f} μV\")\n",
    "        print(f\"   ⚡ Firing rate (good units): {good_units['fr_Hz'].mean():.2f} ± {good_units['fr_Hz'].std():.2f} Hz\")\n",
    "        print(f\"   🎯 Presence ratio (good units): {good_units['presenceRatio'].mean():.3f} ± {good_units['presenceRatio'].std():.3f}\")\n",
    "        print(f\"   🚫 RPV contamination (good units): {good_units['fractionRPVs_estimatedTauR'].mean()*100:.2f}% ± {good_units['fractionRPVs_estimatedTauR'].std()*100:.2f}%\")\n",
    "    \n",
    "    # Check UnitMatch readiness\n",
    "    raw_waveforms_dir = Path(result['save_path']) / \"RawWaveforms\"\n",
    "    if raw_waveforms_dir.exists():\n",
    "        waveform_files = list(raw_waveforms_dir.glob(\"Unit*_RawSpikes.npy\"))\n",
    "        print(f\"\\n🎯 UnitMatch Readiness:\")\n",
    "        print(f\"   ✅ Raw waveforms extracted: {len(waveform_files)} units\")\n",
    "        print(f\"   📁 Location: {raw_waveforms_dir}\")\n",
    "        \n",
    "        # Check file sizes to verify extraction quality\n",
    "        if waveform_files:\n",
    "            sample_file = np.load(waveform_files[0])\n",
    "            print(f\"   📏 Waveform dimensions: {sample_file.shape} (channels × time × spikes)\")\n",
    "            print(f\"   💾 Expected ~1000 spikes per unit for UnitMatch\")\n",
    "    else:\n",
    "        print(f\"\\n⚠️  UnitMatch waveforms not found - may need raw data access\")\n",
    "    \n",
    "    # Display sample of results\n",
    "    print(f\"\\n📋 Sample Quality Metrics (first 5 units):\")\n",
    "    display_cols = ['cluster_id', 'unit_type', 'amplitude', 'fr_Hz', 'presenceRatio', 'fractionRPVs_estimatedTauR']\n",
    "    display(df[display_cols].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🎨 Create BombCell Summary Plots\n",
    "print(\"🎨 Generating BombCell summary plots...\")\n",
    "\n",
    "for i, result in enumerate(session_results):\n",
    "    print(f\"\\n📊 Plotting results for Session {i+1}\")\n",
    "    \n",
    "    # Load the saved results for plotting\n",
    "    quality_metrics = result['quality_metrics']\n",
    "    param = result['param']\n",
    "    \n",
    "    # Create summary plots using BombCell's built-in plotting\n",
    "    try:\n",
    "        fig = bc.plot_summary_data(param, quality_metrics, \n",
    "                                   save_figure=True, \n",
    "                                   save_path=result['save_path'])\n",
    "        plt.show()\n",
    "        print(f\"   ✅ Summary plots saved to: {result['save_path']}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ⚠️  Error creating plots: {e}\")\n",
    "        \n",
    "        # Create basic plots manually if automated plotting fails\n",
    "        df = pd.DataFrame(quality_metrics)\n",
    "        df['unit_type'] = result['unit_type_string']\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "        fig.suptitle(f\"Session {i+1}: BombCell Quality Metrics\", fontsize=14)\n",
    "        \n",
    "        # Amplitude distribution by unit type\n",
    "        for unit_type in df['unit_type'].unique():\n",
    "            subset = df[df['unit_type'] == unit_type]\n",
    "            axes[0,0].hist(subset['amplitude'], alpha=0.6, label=unit_type, bins=20)\n",
    "        axes[0,0].set_xlabel('Amplitude (μV)')\n",
    "        axes[0,0].set_ylabel('Count')\n",
    "        axes[0,0].set_title('Amplitude Distribution')\n",
    "        axes[0,0].legend()\n",
    "        \n",
    "        # Firing rate vs presence ratio\n",
    "        colors = {'GOOD': 'green', 'MUA': 'orange', 'NOISE': 'red', 'NON-SOMA': 'black'}\n",
    "        for unit_type in df['unit_type'].unique():\n",
    "            subset = df[df['unit_type'] == unit_type]\n",
    "            axes[0,1].scatter(subset['presenceRatio'], subset['fr_Hz'], \n",
    "                            alpha=0.6, label=unit_type, c=colors.get(unit_type, 'gray'))\n",
    "        axes[0,1].set_xlabel('Presence Ratio')\n",
    "        axes[0,1].set_ylabel('Firing Rate (Hz)')\n",
    "        axes[0,1].set_title('Firing Rate vs Presence Ratio')\n",
    "        axes[0,1].legend()\n",
    "        \n",
    "        # RPV contamination\n",
    "        rpv_data = df['fractionRPVs_estimatedTauR'] * 100\n",
    "        rpv_data = rpv_data[~np.isnan(rpv_data)]\n",
    "        axes[1,0].hist(rpv_data, bins=20, alpha=0.7)\n",
    "        axes[1,0].set_xlabel('RPV Contamination (%)')\n",
    "        axes[1,0].set_ylabel('Count')\n",
    "        axes[1,0].set_title('Refractory Period Violations')\n",
    "        \n",
    "        # Unit type pie chart\n",
    "        type_counts = df['unit_type'].value_counts()\n",
    "        axes[1,1].pie(type_counts.values, labels=type_counts.index, autopct='%1.1f%%')\n",
    "        axes[1,1].set_title('Unit Type Distribution')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "print(\"🎨 Plotting complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 Part 2: UnitMatch Cross-Session Tracking\n",
    "\n",
    "**What UnitMatch does:**\n",
    "- Compares units across different recording sessions using multiple features\n",
    "- Uses machine learning (Naive Bayes) to calculate match probabilities  \n",
    "- Identifies the same neurons recorded on different days\n",
    "- Provides tools for manual curation and validation\n",
    "\n",
    "**This section requires multiple sessions with BombCell outputs!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔗 Setup UnitMatch Parameters and Paths\n",
    "\n",
    "print(\"🎯 Setting up UnitMatch for cross-session tracking...\")\n",
    "\n",
    "# Get default UnitMatch parameters\n",
    "um_param = default_params.get_default_param()\n",
    "\n",
    "# Set up paths from our BombCell results\n",
    "KS_dirs = [result['ks_dir'] for result in session_results]\n",
    "um_param['KS_dirs'] = KS_dirs\n",
    "\n",
    "# Set up BombCell paths - using exact paths from processing_playground\n",
    "custom_bombcell_paths = [\n",
    "    r'/home/jf5479/cup/Chris/data/cta_backwards/calca_302/2023-04-19/cz_npxl_g0/cz_npxl_g0_imec0/bombcell_testing_jf/cluster_bc_unitType.tsv',\n",
    "    r'/home/jf5479/cup/Chris/data/cta_backwards/calca_302/2023-04-20/cz_npxl_g0/cz_npxl_g0_imec0/bombcell_testing_jf/cluster_bc_unitType.tsv'\n",
    "]\n",
    "\n",
    "custom_raw_waveform_paths = [\n",
    "    r'/home/jf5479/cup/Chris/data/cta_backwards/calca_302/2023-04-19/cz_npxl_g0/cz_npxl_g0_imec0/bombcell_testing_jf/RawWaveforms',\n",
    "    r'/home/jf5479/cup/Chris/data/cta_backwards/calca_302/2023-04-20/cz_npxl_g0/cz_npxl_g0_imec0/bombcell_testing_jf/RawWaveforms'\n",
    "]\n",
    "\n",
    "print(f\"📁 Kilosort directories: {len(KS_dirs)}\")\n",
    "for i, ks_dir in enumerate(KS_dirs):\n",
    "    print(f\"   Session {i+1}: {Path(ks_dir).name}\")\n",
    "\n",
    "print(f\"📊 BombCell unit classifications:\")\n",
    "for i, bc_path in enumerate(custom_bombcell_paths):\n",
    "    exists = \"✅\" if Path(bc_path).exists() else \"❌\"\n",
    "    print(f\"   Session {i+1}: {exists} {Path(bc_path).name}\")\n",
    "    \n",
    "print(f\"🎯 Raw waveforms for UnitMatch:\")\n",
    "for i, wv_path in enumerate(custom_raw_waveform_paths):\n",
    "    exists = \"✅\" if Path(wv_path).exists() else \"❌\"\n",
    "    n_files = len(list(Path(wv_path).glob(\"Unit*_RawSpikes.npy\"))) if Path(wv_path).exists() else 0\n",
    "    print(f\"   Session {i+1}: {exists} {n_files} waveform files\")\n",
    "\n",
    "# Setup UnitMatch paths - this matches exactly what processing_playground does\n",
    "try:\n",
    "    wave_paths, unit_label_paths, channel_pos = util.paths_from_KS(\n",
    "        KS_dirs, \n",
    "        custom_raw_waveform_paths=custom_raw_waveform_paths,\n",
    "        custom_bombcell_paths=custom_bombcell_paths\n",
    "    )\n",
    "    \n",
    "    um_param = util.get_probe_geometry(channel_pos[0], um_param)\n",
    "    print(\"✅ UnitMatch paths configured successfully\")\n",
    "    UNITMATCH_READY = True\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error setting up UnitMatch paths: {e}\")\n",
    "    print(\"   Make sure BombCell has been run and waveforms extracted\")\n",
    "    UNITMATCH_READY = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🧠 Run UnitMatch Analysis Pipeline\n",
    "\n",
    "if UNITMATCH_READY:\n",
    "    print(\"🚀 Running UnitMatch cross-session analysis...\")\n",
    "    \n",
    "    # Step 0: Load good units and waveform data\n",
    "    print(\"📊 Step 0: Loading unit waveforms...\")\n",
    "    waveform, session_id, session_switch, within_session, good_units, um_param = util.load_good_waveforms(\n",
    "        wave_paths, unit_label_paths, um_param, good_units_only=True\n",
    "    ) \n",
    "    \n",
    "    print(f\"   ✅ Loaded {len(np.concatenate(good_units))} good units across {len(session_results)} sessions\")\n",
    "    print(f\"   📏 Waveform shape: {waveform.shape} (units × channels × time)\")\n",
    "    \n",
    "    # Create cluster info\n",
    "    clus_info = {\n",
    "        'good_units': good_units, \n",
    "        'session_switch': session_switch, \n",
    "        'session_id': session_id,\n",
    "        'original_ids': np.concatenate(good_units)\n",
    "    }\n",
    "    \n",
    "    # Step 1: Extract waveform parameters\n",
    "    print(\"🔍 Step 1: Extracting waveform features...\")\n",
    "    extracted_wave_properties = ov.extract_parameters(waveform, channel_pos, clus_info, um_param)\n",
    "    print(\"   ✅ Extracted amplitude, spatial decay, and waveform features\")\n",
    "    \n",
    "    # Steps 2-4: Calculate similarity metrics with drift correction\n",
    "    print(\"📐 Steps 2-4: Computing similarity metrics and drift correction...\")\n",
    "    total_score, candidate_pairs, scores_to_include, predictors = ov.extract_metric_scores(\n",
    "        extracted_wave_properties, session_switch, within_session, um_param, niter=2\n",
    "    )\n",
    "    print(f\"   ✅ Found {np.sum(candidate_pairs)} candidate unit pairs\")\n",
    "    print(f\"   📊 Metrics included: {list(scores_to_include.keys())}\")\n",
    "    \n",
    "    # Step 5: Probability analysis using Naive Bayes\n",
    "    print(\"🧮 Step 5: Calculating match probabilities...\")\n",
    "    \n",
    "    # Set up priors\n",
    "    prior_match = 1 - (um_param['n_expected_matches'] / um_param['n_units']**2)\n",
    "    priors = np.array([prior_match, 1 - prior_match])\n",
    "    \n",
    "    # Train Naive Bayes classifier\n",
    "    labels = candidate_pairs.astype(int)\n",
    "    cond = np.unique(labels)\n",
    "    parameter_kernels = bf.get_parameter_kernels(scores_to_include, labels, cond, um_param, add_one=1)\n",
    "    \n",
    "    # Calculate match probabilities\n",
    "    probability = bf.apply_naive_bayes(parameter_kernels, priors, predictors, um_param, cond)\n",
    "    output_prob_matrix = probability[:, 1].reshape(um_param['n_units'], um_param['n_units'])\n",
    "    \n",
    "    print(f\"   ✅ Calculated probability matrix: {output_prob_matrix.shape}\")\n",
    "    print(f\"   📈 Max probability: {np.max(output_prob_matrix):.3f}\")\n",
    "    print(f\"   📊 Mean probability: {np.mean(output_prob_matrix):.3f}\")\n",
    "    \n",
    "else:\n",
    "    print(\"⏭️  Skipping UnitMatch analysis - requirements not met\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📊 Analyze UnitMatch Results\n",
    "\n",
    "if UNITMATCH_READY:\n",
    "    print(\"📈 Analyzing UnitMatch results...\")\n",
    "    \n",
    "    # Evaluate matching performance\n",
    "    match_threshold = um_param.get('match_threshold', 0.75)\n",
    "    util.evaluate_output(output_prob_matrix, um_param, within_session, session_switch, \n",
    "                        match_threshold=match_threshold)\n",
    "    \n",
    "    # Create binary match matrix\n",
    "    output_threshold = np.zeros_like(output_prob_matrix)\n",
    "    output_threshold[output_prob_matrix > match_threshold] = 1\n",
    "    \n",
    "    # Count matches\n",
    "    total_matches = np.sum(output_threshold)\n",
    "    within_session_matches = np.sum(output_threshold * within_session)\n",
    "    cross_session_matches = total_matches - within_session_matches\n",
    "    \n",
    "    print(f\"\\n🎯 Match Summary (threshold = {match_threshold}):\")\n",
    "    print(f\"   🔗 Total matches found: {total_matches}\")\n",
    "    print(f\"   📍 Within-session matches: {within_session_matches}\")\n",
    "    print(f\"   🌉 Cross-session matches: {cross_session_matches}\")\n",
    "    \n",
    "    # Visualize probability matrix\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Probability matrix\n",
    "    im1 = axes[0].imshow(output_prob_matrix, cmap='viridis', aspect='auto')\n",
    "    axes[0].set_title('Unit Match Probability Matrix')\n",
    "    axes[0].set_xlabel('Unit Index')\n",
    "    axes[0].set_ylabel('Unit Index')\n",
    "    plt.colorbar(im1, ax=axes[0], label='Match Probability')\n",
    "    \n",
    "    # Session boundaries\n",
    "    n_units_cumsum = np.cumsum([0] + [len(units) for units in good_units])\n",
    "    for boundary in n_units_cumsum[1:-1]:\n",
    "        axes[0].axhline(boundary, color='red', linestyle='--', alpha=0.7)\n",
    "        axes[0].axvline(boundary, color='red', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Binary matches\n",
    "    im2 = axes[1].imshow(output_threshold, cmap='Greys', aspect='auto')\n",
    "    axes[1].set_title(f'Matches Above Threshold ({match_threshold})')\n",
    "    axes[1].set_xlabel('Unit Index')\n",
    "    axes[1].set_ylabel('Unit Index')\n",
    "    \n",
    "    # Session boundaries\n",
    "    for boundary in n_units_cumsum[1:-1]:\n",
    "        axes[1].axhline(boundary, color='red', linestyle='--', alpha=0.7)\n",
    "        axes[1].axvline(boundary, color='red', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"📊 Red dashed lines show session boundaries\")\n",
    "    print(\"🎯 Diagonal represents within-session matches\")\n",
    "    print(\"🌉 Off-diagonal represents cross-session matches\")\n",
    "    \n",
    "else:\n",
    "    print(\"⏭️  No UnitMatch results to analyze\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🎮 Interactive Manual Curation with UnitMatch GUI\n",
    "\n",
    "if UNITMATCH_READY:\n",
    "    print(\"🎮 Setting up interactive curation tools...\")\n",
    "    \n",
    "    # Prepare data for GUI - extract all the required variables\n",
    "    amplitude = extracted_wave_properties['amplitude']\n",
    "    spatial_decay = extracted_wave_properties['spatial_decay']\n",
    "    avg_centroid = extracted_wave_properties['avg_centroid']\n",
    "    avg_waveform = extracted_wave_properties['avg_waveform']\n",
    "    avg_waveform_per_tp = extracted_wave_properties['avg_waveform_per_tp']\n",
    "    wave_idx = extracted_wave_properties['good_wave_idxs']\n",
    "    max_site = extracted_wave_properties['max_site']\n",
    "    max_site_mean = extracted_wave_properties['max_site_mean']\n",
    "    \n",
    "    # Process info for GUI\n",
    "    um_gui.process_info_for_GUI(\n",
    "        output_prob_matrix, match_threshold, scores_to_include, total_score, \n",
    "        amplitude, spatial_decay, avg_centroid, avg_waveform, avg_waveform_per_tp, \n",
    "        wave_idx, max_site, max_site_mean, waveform, within_session, \n",
    "        channel_pos, clus_info, um_param\n",
    "    )\n",
    "    \n",
    "    print(\"✅ GUI data prepared successfully\")\n",
    "    print(\"\\n🎯 To launch interactive curation:\")\n",
    "    print(\"   Run: is_match, not_match, matches_GUI = um_gui.run_GUI()\")\n",
    "    print(\"\\n🔍 The GUI provides:\")\n",
    "    print(\"   - Interactive visualization of unit pairs\")\n",
    "    print(\"   - Side-by-side waveform comparisons\")\n",
    "    print(\"   - Quality metric displays\")\n",
    "    print(\"   - Manual accept/reject controls\")\n",
    "    print(\"   - Real-time match probability updates\")\n",
    "    \n",
    "    print(\"\\n💡 Curation workflow:\")\n",
    "    print(\"   1. GUI shows potential matches above threshold\")\n",
    "    print(\"   2. Review waveform similarity and spatial locations\")\n",
    "    print(\"   3. Accept good matches, reject false positives\")\n",
    "    print(\"   4. Use curated results for final unit tracking\")\n",
    "    \n",
    "else:\n",
    "    print(\"⏭️  GUI setup skipped - UnitMatch analysis not ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 💾 Save Results and Generate Final Outputs\n",
    "\n",
    "if UNITMATCH_READY:\n",
    "    print(\"💾 Saving UnitMatch results and generating final outputs...\")\n",
    "    \n",
    "    # Assign unique IDs to matched units across sessions\n",
    "    UIDs = aid.assign_unique_id(output_prob_matrix, um_param, clus_info)\n",
    "    \n",
    "    # Get final matches above threshold\n",
    "    matches = np.argwhere(output_threshold == 1)\n",
    "    \n",
    "    # Create save directory next to the first session's BombCell results\n",
    "    save_dir = Path(session_results[0]['save_path']).parent / \"UnitMatch_Results\"\n",
    "    save_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    print(f\"📁 Saving results to: {save_dir}\")\n",
    "    \n",
    "    # Save comprehensive UnitMatch results\n",
    "    su.save_to_output(\n",
    "        str(save_dir), \n",
    "        scores_to_include, \n",
    "        matches,\n",
    "        output_prob_matrix, \n",
    "        avg_centroid, \n",
    "        avg_waveform, \n",
    "        avg_waveform_per_tp, \n",
    "        max_site,\n",
    "        total_score, \n",
    "        output_threshold, \n",
    "        clus_info, \n",
    "        um_param, \n",
    "        UIDs=UIDs, \n",
    "        matches_curated=None,  # Set to matches_curated if manual curation was performed\n",
    "        save_match_table=True\n",
    "    )\n",
    "    \n",
    "    print(\"✅ Results saved successfully!\")\n",
    "    \n",
    "    # Generate summary statistics\n",
    "    n_unique_neurons = len(np.unique(UIDs))\n",
    "    n_total_units = len(np.concatenate(good_units))\n",
    "    n_cross_session_matches = cross_session_matches\n",
    "    tracking_efficiency = (n_cross_session_matches / min([len(units) for units in good_units])) * 100\n",
    "    \n",
    "    print(f\"\\n📊 Final Results Summary:\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"🧠 Dataset: calca_302 (2 sessions)\")\n",
    "    print(f\"📊 Total units analyzed: {sum(len(result['unit_type']) for result in session_results)}\")\n",
    "    print(f\"✅ Good units tracked: {n_total_units}\")\n",
    "    print(f\"🔗 Cross-session matches found: {n_cross_session_matches}\")\n",
    "    print(f\"🏷️  Unique neurons identified: {n_unique_neurons}\")\n",
    "    print(f\"📈 Tracking efficiency: {tracking_efficiency:.1f}%\")\n",
    "    \n",
    "    print(f\"\\n📁 Output Files Generated:\")\n",
    "    print(f\"   📊 output_prob_matrix.npy - Match probability matrix\")\n",
    "    print(f\"   📋 match_table.csv - Detailed match information\")\n",
    "    print(f\"   🏷️  unique_IDs.npy - Cross-session unit identifiers\")\n",
    "    print(f\"   📈 total_score.npy - Combined similarity scores\")\n",
    "    print(f\"   🎯 clus_info.json - Unit metadata and session info\")\n",
    "    \n",
    "    print(f\"\\n🎯 Next Steps:\")\n",
    "    print(f\"   1. Review match probabilities and validate results\")\n",
    "    print(f\"   2. Use unique IDs for longitudinal analysis\")\n",
    "    print(f\"   3. Analyze stability of tracked neurons\")\n",
    "    print(f\"   4. Examine changes in firing patterns across sessions\")\n",
    "    \n",
    "else:\n",
    "    print(\"⏭️  No UnitMatch results to save - analysis was not completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎉 Pipeline Complete!\n",
    "\n",
    "\n",
    "### Key Files Generated\n",
    "\n",
    "**BombCell Outputs (per session):**\n",
    "- `qMetrics/cluster_bc_unitType.tsv` - Unit classifications\n",
    "- `qMetrics/templates._bc_qMetrics.parquet` - Quality metrics\n",
    "- `qMetrics/RawWaveforms/` - Raw spike waveforms for UnitMatch\n",
    "- `qMetrics/summary_plots.png` - Quality control visualizations\n",
    "\n",
    "**UnitMatch Outputs:**\n",
    "- `UnitMatch_Results/output_prob_matrix.npy` - Match probabilities\n",
    "- `UnitMatch_Results/match_table.csv` - Unit matches and scores\n",
    "- `UnitMatch_Results/unique_IDs.npy` - Cross-session unit identifiers\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ephys",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
